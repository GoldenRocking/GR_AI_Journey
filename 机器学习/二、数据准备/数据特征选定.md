### 数据特征选定：

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

特征过程的本质就是一项工程活动，目的是最大限度地从原始数据中提取合适的特征，以供算法和模型使用。

**数据特征选择的方法：**

* 单变量特征选定
* 递归特征消除
* 主要成分分析
* 特征的重要性



### 特征选定：

特征选定是一个流程，能够选择有助于提高预测结果准确度的特征数据，或者有助于发现我们感兴趣的输出结果的特征数据。

如果数据中包含无关的特征属性，会降低算法的准确度，对预测新数据造成干扰，尤其是线性相关算法。

**好处：**

* **降低数据的拟合度:**较少的冗余数据，会使算法得出结论的机会更大
* **提高算法精度：**较少的误导数据，能够提高算法的准确度
* **减少训练时间：**越少的数据，训练模型所需要的时间就越少



**单变量特征选定**：

* 统计分析可以用来分析选择对结果影响最大的数据特征
* 卡方检验是检验定性自变量对定性因变量的相关性的方法
* 卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，偏离程度决定了卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合



**递归特征消除：**

递归特征消除（RFE）使用一个基模型来进行多轮训练，每轮训练后消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

通过每一个基模型的精度，找到对最终的预测结果影响最大的数据特征。



**主要成分分析：**

通过每一个基模型的精度，找到对最终的预测结果影响最大的数据特征。

常见的降维方法除了主要成分分析（PCA），还有线性判别分析（LDA），它本身也是一个分类模型。

PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；

而LDA是为了让映射后的样本有最好的分类性能。

PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。

在聚类算法中，通常会利用PCA对数据进行降维处理，以利于对数据的简化分析和可视化。



**特征重要性：**

袋装决策树算法（Bagged Decision Tress）、随机森林算法和极端随机树算法都可以用来计算数据特征的重要性。